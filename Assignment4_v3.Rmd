---
title: "Assignment 4: Performance Comparison of Different Approaches"
author: "Nazlı Gül"
date: "29/01/2021"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message = FALSE, warning = FALSE, error = FALSE)
```

<style>
#TOC {
 color: 
 font-family: Calibri;
 background-color:
 border-color: darkred;
}
#header {
 color: darkred;
 font-family: Calibri;
 background-color:
}
body {
 font-family: Calibri;
 }
 
</style> 

# Performance Comparison

## 1. Introduction

The aim of this assignment is to compare the performance of different approaches namely, Penalized Regression Approach, Decision Tree, Random Forrest, and Stochastic Gradient Boosting. Several packages are used in this assignment such as _data.table_, _tidyverse_, _glmnet_, _caret_, _gbm_, _rpart_, , _rattle_,and _randomForest_. purposes.

```{r packages, message=FALSE, warning=FALSE}
library(data.table)
library(tidyverse)
library(glmnet)
library(caret)
library(gbm)
library(plyr)
library(skimr)
library(dplyr)
library(rpart)
library(randomForest)
library(gbm)
library(plyr)

setwd("C:/Users/lenovo/Documents/GitHub/fall20-NazliGul")
```

## 2. Datasets

Four datasets for a classification task from different domanins are found before moving to model constructions. These datasets can be seen below with the features they include. Out of four, three of them are downloaded from **Kaggle** and the last one is downloaded from **UCI** machine learning repository. These datasets are adjusted as all of them have - a separate labeled test data
- more than 200 instances in both training and test samples
- more than 20 features
- a brief description of the dataset and features. 

#### Dataset 1: Baseball Player Statistics (Regression Problem with +50 Features)

Sports analytics is a booming field. Owners, coaches, and fans are using statistical measures and models of all kinds to study the performance of players and teams. A very simple example is provided by the study of yearly data on batting averages for individual players in the sport of baseball. The sample used in this dataset contains **4535 rows** of data for a select group of players during the years 1960-2004, and it was obtained from the Lahman Baseball Database.  The objective of this exercise will be to predict a player’s batting average in a given year from his batting average in the previous year and/or his cumulative batting average over all previous years for which data is available. In total there are 81 features in this dataset, first 50 features can be seen below.The dataset can be downloaded from this [link](https://www.kaggle.com/akhilups/baseball-player-statistics).

**Task:** Regression Problem with +50 Features<br>
**Variables:** 81<br>
**Instances:** 4535<br>

- YEAR:	Year (1959-2004) 
- YRINDEX: Year index (1959=1) 
- PLAYERID:	Unique player ID 
- NAMElast:	Last name 
- NAMEfirst:	First name 
- TEAM:	Team(s) played on that year 
- LG:	League(s) played in that year 
- LGCODE:	League code (0=AL,1=both, 2=NL) 
- G:	Games 
- AB:	At bats (number of turns at bat)
- R: Runs (player advances around all 4 bases and scores)
- H:	Hits (player hits a fair ball and reaches base safely without losing a runner already on base)
- HR:	Home runs (balls that fly out of the ballpark and allow the batter to run all the way home)
- RBI: Runs batted in (number of runners who score on the basis of a player’s hits)
- TB:	Total bases (=H+DB+2TR+3HR) 
- OB:	On base (=H+BB+HBP) 
- PA:	Plate appearances (=H+BB+HBP+SF) 
- DBL:	Doubles (long hits that allow the batter to run all the way to second base)
- TR:	Triples (very long hits that allow the batter to run all the way to third base)
- SB:	Stolen bases (runner “steals” the next base by running to it when no one is looking)
- CS:	Caught stealing (runner is tagged out with the ball while attempting to steal a base)
- BB:	Bases on balls (player “walks” to first base if the pitcher throws 4 bad pitches) 
- SO:	Struck out (player is out by swinging-and-missing and/or not-swinging-at-good-pitch 3 times)
- IBB:	Intentional bases on balls (pitcher deliberately throws 4 bad pitches to avoid risking a hit) 
- HBP:	Hit by pitch (player gets to go to first base on the basis of getting hit by pitch)
- SH:	Sacrifices (player hits the ball and gets himself out but enables another runner to advance)
- SF:	Sacrifice flies (sacrifices that are balls caught in the air in the outfield)
- GIDP:	Grounded into double play (player hits a ball that gets himself out and also another runner)
- AVG:	Batting average (=H/AB) 
- OBP:	On base percentage (=OB/PA) 
- SLG:	Slugging percentage (=TB/AB) 
- AVGcum:	Cumulative batting average of the same player (in his career since 1959) 
- OBPcum:	Cumulative on-base percentage 
- SLGcum:	Cumulative slugging percentage 
- ABcum:	Cumulative total at-bats 
- Rcum:	Cumulative total runs 
- Hcum:	Cumulative total hits 
- HRcum:	Cumulative total home runs 
- RBIcum:	Cumulative total runs batted in 
- PAcum:	Cumulative total plate appearances 
- OBcum:	Cumulative total of total-on-base 
- TBcum:	Cumulative total of total-bases 
- EXP:	Experience (# years after and including the first year in which CPA>50) 
- PAYR:	Plate appearances per year of experience (since 1959) 
- MLAVG:	Major league batting average (same year, same set of players) 
- MLOBP:	Major league on base percentage 
- MLSLG:	Major league slugging average 
- MLRAVG:	Major league average runs per plate appearance 
- MLHRAVG:	Major league average home runs per plate appearance 
- MLRBIAVG: Major league average RBI's per plate appearance 

#### Dataset 2: Online News Popularity (Regression Problem with +50 Features)

This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. There are many features about articles such as number of words, best and worst keywords, shares. the dataset includes both numeric and binary variables. The target is the number of shares in social networks. The dataset can be downloaded from this [link](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#).

**Task:** Regression Problem with +50 Features<br>
**Variables:** 60<br>
**Instances:** 39644<br>

- url: URL of the article (non-predictive)
- timedelta: Days between the article publication and the dataset acquisition (non-predictive)
- n_tokens_title: Number of words in the title
- n_tokens_content: Number of words in the content
- n_unique_tokens: Rate of unique words in the content
- n_non_stop_words: Rate of non-stop words in the content
- n_non_stop_unique_tokens: Rate of unique non-stop words in the content
- num_hrefs: Number of links
- num_self_hrefs: Number of links to other articles published by Mashable
- num_imgs: Number of images
- num_videos: Number of videos
- average_token_length: Average length of the words in the content
- num_keywords: Number of keywords in the metadata
- data_channel_is_lifestyle: Is data channel 'Lifestyle'?
- data_channel_is_entertainment: Is data channel 'Entertainment'?
- data_channel_is_bus: Is data channel 'Business'?
- data_channel_is_socmed: Is data channel 'Social Media'?
- data_channel_is_tech: Is data channel 'Tech'?
- data_channel_is_world: Is data channel 'World'?
- kw_min_min: Worst keyword (min. shares)
- kw_max_min: Worst keyword (max. shares)
- kw_avg_min: Worst keyword (avg. shares)
- kw_min_max: Best keyword (min. shares)
- kw_max_max: Best keyword (max. shares)
- kw_avg_max: Best keyword (avg. shares)
- kw_min_avg: Avg. keyword (min. shares)
- kw_max_avg: Avg. keyword (max. shares)
- kw_avg_avg: Avg. keyword (avg. shares)
- self_reference_min_shares: Min. shares of referenced articles in Mashable
- self_reference_max_shares: Max. shares of referenced articles in Mashable
- self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
- weekday_is_monday: Was the article published on a Monday?
- weekday_is_tuesday: Was the article published on a Tuesday?
- weekday_is_wednesday: Was the article published on a Wednesday?
- weekday_is_thursday: Was the article published on a Thursday?
- weekday_is_friday: Was the article published on a Friday?
- weekday_is_saturday: Was the article published on a Saturday?
- weekday_is_sunday: Was the article published on a Sunday?
- is_weekend: Was the article published on the weekend?
- LDA_00: Closeness to LDA topic 0
- LDA_01: Closeness to LDA topic 1
- LDA_02: Closeness to LDA topic 2
- LDA_03: Closeness to LDA topic 3
- LDA_04: Closeness to LDA topic 4
- global_subjectivity: Text subjectivity
- global_sentiment_polarity: Text sentiment polarity
- global_rate_positive_words: Rate of positive words in the content
- global_rate_negative_words: Rate of negative words in the content
- rate_positive_words: Rate of positive words among non-neutral tokens
- rate_negative_words: Rate of negative words among non-neutral tokens
- avg_positive_polarity: Avg. polarity of positive words
- min_positive_polarity: Min. polarity of positive words
- max_positive_polarity: Max. polarity of positive words
- avg_negative_polarity: Avg. polarity of negative words
- min_negative_polarity: Min. polarity of negative words
- max_negative_polarity: Max. polarity of negative words
- title_subjectivity: Title subjectivity
- title_sentiment_polarity: Title polarity
- abs_title_subjectivity: Absolute subjectivity level
- abs_title_sentiment_polarity: Absolute polarity level
- shares: Number of shares (target)

#### Dataset 3: Mobile Price Segmentation (Multiclass Classification Problem)

There are several features which affect the price of a mobile phone. In this competitive market, in order to solve pricing problem sales data of mobile phones from different companies are used. In this dataset, the actual prices are not included, whereas a price range is used in order to indicate how high the price is. The dataset can be downloaded from this [link](https://www.kaggle.com/iabhishekofficial/mobile-price-classification).

**Task:** Multiclass Classification Problem<br>
**Variables:** 20<br>
**Instances:** 3000<br>

- battery_power: Total energy a battery can store in one time measured in mAh
- blue: Has bluetooth or not
- clock_speed: Speed at which microprocessor executes instructions
- dual_sim: Has dual sim support or not
- fc: Front camera mega pixels
- four_g: Has 4G or not
- int_memory: Internal memory in gigabytes
- m_dep: Mobile Depth in cm
- mobile_wt: Weight of mobile phone
- n_cores: Number of cores of processor
- pc: Primary camera mega pixels
- px_height: Pixel resolution height
- px_width: Pixel resolution width
- ram: Random access memory in megabytes
- sc_h: Screen height of mobile in cm
- sc_w: Screen width of mobile in cm
- talk_time: Longest time that a single battery charge will last when you are talking
- three_g: Has 3G or not
- touch_screen: Has touch screen or not
- wifi: Has wifi or not
- price_range: The target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).

#### Dataset 4: Telco User Churn (Class Imbalance Problem)

It is important to predict the behavior of customers in order to retain customers. There are many relevant features in predicting the loyalty of a customer. From this dataset we can gain insights from certain features of the customers. Each row represents a customer, and each column contains customer's attributes described below. The dataset can be downloaded from this [link](https://www.kaggle.com/blastchar/telco-customer-churn).

**Task:** Class Imbalance Problem<br>
**Variables:** 20<br>
**Instances:** 7043<br>

- Customer ID: Customer ID
- Gender: Whether the customer is a male or a female
- SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)
- Partner: Whether the customer has a partner or not (Yes, No)
- Dependents: Whether the customer has dependents or not (Yes, No)
- Tenure: Number of months the customer has stayed with the company
- PhoneService: Whether the customer has a phone service or not (Yes, No)
- MultipleLines: Whether the customer has multiple lines or not (Yes, No, No phone service)
- InternetService: Customer’s internet service provider (DSL, Fiber optic, No)
- OnlineSecurity: Whether the customer has online security or not (Yes, No, No internet service)
- OnlineBackup: Whether the customer has online backup or not (Yes, No, No internet service)
- DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)
- TechSupport: Whether the customer has tech support or not (Yes, No, No internet service)
- StreamingTV: Whether the customer has streaming TV or not (Yes, No, No internet service)
- StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)
- Contract: The contract term of the customer (Month-to-month, One year, Two year)
- PaperlessBilling: Whether the customer has paperless billing or not (Yes, No)
- PaymentMethod: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
- MonthlyCharges: The amount charged to the customer monthly
- TotalCharges: The total amount charged to the customer
- Churn: Whether the customer churned or not (Yes or No)

## 3. Tasks

Specifications for the algorithms:
- **Penalized Regression Approaches (PRA):** Penalized regression approaches with Lasso Penalty is used, and parameter lambda is tuned.
- **Decision Trees (DT):** Classification and Regression Trees (CART) for training is used. The criteria to control the depth of the tree are *the minimal number of observations per tree leaf*, and * complexity parameter*.
- **Random Forests (RF):** Random Forest is used for training. Each tree is basically a classification and regression tree however the data used to train each tree is a random subsample of the whole training data, and not all features are evaluated at each split decision. Trees are grown until *the minimal number of observations per tree leaf* is observed.
- **Stochastic Gradient Boosting (SGB):** Gradient Boosted Tree is used for training. The main interests in this approach are *depth of the tree*, *learning rate*, and *number of trees*. Inherited from the tree base learning, there is also *the minimal number of observations per tree leaf*. 

When we want to calculate a metric to compare any two models, we will use _calculate_mape_ for numerical target and _calculate_cm_ for categorical target. When we are trying to find the best lambda parameter for Lasso Regression, we will use _lasso_lambdas_ function. When we want to create a Lasso model, we will use _lasso_model_ function. When we want to tune hyper parameters for other algorithms, we will use _model_tuning_ function.

```{r functions}
calculate_mape = function(pred, true){
  mean(abs(pred - true)/true) * 100
}

calculate_cm = function(pred, true){
  sum(diag(table(true, pred))) / length(true)
}

lasso_lambdas = function(data, target, family){
  cols = setdiff(names(data), target)
  X = data.matrix(data[, ..cols, with = FALSE])
  y = data.matrix(data[,..target, with = FALSE])

  lasso_cv = cv.glmnet(X, y, family = family, standardize = TRUE)
  lambda_min = lasso_cv$lambda.min
  lambda_1se = lasso_cv$lambda.1se
  c(lambda_min, lambda_1se)
}

lasso_model = function(data, target, lambda, family){
  cols = setdiff(names(data), target)
  X = data.matrix(data[, ..cols, with = FALSE])
  y = data.matrix(data[, ..target, with = FALSE])
  
  model = glmnet(X,
                 y,
                 lambda = lambda,
                 family = family,
                 standardize = TRUE,
                 alpha = 1)
  model
}

dt_tuning = function(data, target, min_bucket){
  set.seed(12345) 
  TRcontrol = trainControl(method ="cv", number = 10)
  
  model = train(as.formula(paste(target, "~ .")),
                data = data,
                method = "rpart",
                trControl = TRcontrol, 
                tuneGrid = expand.grid(cp = seq(0.001, 0.1, length.out = 6)),
                control = rpart.control(min_bucket = min_bucket)
                ) 
  model
}

rf_tuning = function(data, target){
  set.seed(12345) 
  control = trainControl(method ="cv", number = 10)
  
  model = train(as.formula(paste(target, "~ .")),
                data = data,
                trControl = control, 
                tuneGrid =  expand.grid(mtry = seq(floor(ncol(data) / 4), floor(ncol(data) / 2), length.out = 5)),
                ntree = 500,
                nodesize = 5 
                ) 
  model
}

gbm_tuning = function(data, target, distribution, grid){
  for(i in 1:nrow(grid)) {
    set.seed(12345)
    
    gbm_model <- gbm(as.formula(paste(target, "~ .")),
                     data = data,
                     distribution = distribution,
                     interaction.depth = grid$interaction.depth[i],
                     n.trees = grid$n.trees[i],
                     shrinkage = grid$shrinkage[i],
                     n.minobsinnode = grid$n.minobsinnode[i],
                     train.fraction = .75,
                     n.cores = NULL,
                     verbose = FALSE
    )
    grid$min_RMSE[i] <- sqrt(min(gbm_model$valid.error))
  }
  best_param = grid[which.min(grid$min_RMSE),]
  
  model <- gbm(as.formula(paste(target, "~ .")),
                   data = data,
                   distribution = distribution,
                   interaction.depth = best_param$interaction.depth,
                   n.trees = best_param$n.trees,
                   shrinkage = best_param$shrinkage,
                   n.minobsinnode = best_param$n.minobsinnode,
                   train.fraction = .75,
                   n.cores = NULL,
                   verbose = FALSE
  ) 
  model
}
```

### 3.1 Baseball Player Statistics

```{r}
baseball_data = fread("Baseball_Statistics.csv")
colnames(baseball_data) <- make.names(colnames(baseball_data))
baseball_data = baseball_data %>% select(-c(YRINDEX, PLAYERID))
skim(baseball_data)
str(baseball_data)
head(baseball_data)
table(baseball_data$Batting.average)
baseball_data = baseball_data %>%
  mutate_if(is.character, as.factor)

online_news_data = fread("Online_News_Popularity.csv")
colnames(online_news_data) <- make.names(colnames(online_news_data))
online_news_data = online_news_data %>% select(-url)
online_news_data = online_news_data[sample(nrow(online_news_data), 5000), ]
skim(online_news_data)
str(online_news_data)
head(online_news_data)
table(online_news_data$shares)
online_news_data = online_news_data %>%
  mutate_if(is.character, as.factor)

mobile_price_data_train = fread("Mobile_Price_train.csv")
mobile_price_data_test = fread("Mobile_Price_test.csv")
colnames(mobile_price_data_train) <- make.names(colnames(mobile_price_data_train))
colnames(mobile_price_data_test) <- make.names(colnames(mobile_price_data_test))
skim(mobile_price_data_train)
str(mobile_price_data_train)
head(mobile_price_data_train)
table(mobile_price_data_train$price_range)
mobile_price_data_train = mobile_price_data_train %>%
  mutate_if(is.character, as.factor)
mobile_price_data_test = mobile_price_data_test %>%
  mutate_if(is.character, as.factor)

churn_data = fread("Customer_Churn.csv")
colnames(churn_data) <- make.names(colnames(churn_data))
churn_data = churn_data %>% select(-customerID)
skim(churn_data)
str(churn_data)
head(churn_data)
table(churn_data$Churn)
churn_data = churn_data %>%
  mutate_if(is.character, as.factor)
```
In order to build classification models, I have to divide data set into train and test sets. By randomly sampling 70% of the row indices, I have created the training set. The remaining rows forming the test set.

In this step I split my dataset in to training and test groups. While doing that I leave 70% in taining set and 30% in the test set.

```{r}
set.seed(3295)
split_baseball_data <- createDataPartition(baseball_data$`Batting.average`, p = .7, list = FALSE, times = 1)
baseball_data_train <- baseball_data[split_baseball_data,]
baseball_data_test  <- baseball_data[-split_baseball_data,]

set.seed(3295)
split_online_news_data <- createDataPartition(online_news_data$`shares`, p = .7, list = FALSE, times = 1)
online_news_data_train <- online_news_data[split_online_news_data,]
online_news_data_test  <- online_news_data[-split_online_news_data,]

set.seed(3295)
split_churn_data <- createDataPartition(churn_data$`Churn`, p = .7, list = FALSE, times = 1)
churn_data_train <- churn_data[split_churn_data,]
churn_data_test  <- churn_data[-split_churn_data,]
```

Then I have checked for missing and duplicated values.

```{r}
sum(duplicated(baseball_data_train))
sum(duplicated(baseball_data_test))
colSums(apply(baseball_data_train, 2, is.na))
colSums(apply(baseball_data_test, 2, is.na))
sum(apply(baseball_data_train, 2, is.na))
sum(apply(baseball_data_test, 2, is.na))

sum(duplicated(online_news_data_train))
sum(duplicated(online_news_data_test))
colSums(apply(online_news_data_train, 2, is.na))
colSums(apply(online_news_data_test, 2, is.na))
sum(apply(online_news_data_train, 2, is.na))
sum(apply(online_news_data_test, 2, is.na))

sum(duplicated(mobile_price_data_train))
sum(duplicated(mobile_price_data_test))
colSums(apply(mobile_price_data_train, 2, is.na))
colSums(apply(mobile_price_data_test, 2, is.na))
sum(apply(mobile_price_data_train, 2, is.na))
sum(apply(mobile_price_data_test, 2, is.na))

sum(duplicated(churn_data_train))
sum(duplicated(churn_data_test))
colSums(apply(churn_data_train, 2, is.na))
colSums(apply(churn_data_test, 2, is.na))
sum(apply(churn_data_train, 2, is.na))
sum(apply(churn_data_test, 2, is.na))

```


As we can see from above, we have missing values in `Customer Churn` data. we have only 10 rows that have NULL values in the train and 1 row that has NULL value in the test set. So, we can just remove these rows from the data.

```{r}
churn_data_train = na.omit(churn_data_train)
churn_data_test = na.omit(churn_data_test)

sum(apply(churn_data_train, 2, is.na))
sum(apply(churn_data_test, 2, is.na))
```


Based on these results I concluded that the data is clean and I can move on to building models. Now, we are ready to apply algorithms to these datasets. As algorithms, we will use Lasso Regression, Decision Tree, Random Forest and Stochastic Gradient Boosting. After creating all these models, we can compare the results.

#### Penalized Regression Approach  

```{r lasso lambda}
lambda_baseball = lasso_lambdas(baseball_data_train, "Batting.average", "gaussian")
lambda_online_news = lasso_lambdas(online_news_data_train, "shares", "gaussian")
lambda_mobile_price = lasso_lambdas(mobile_price_data_train, "price_range", "multinomial")
lambda_churn = lasso_lambdas(churn_data_train, "Churn", "binomial")

lambda_min_baseball = lambda_baseball[1]
lambda_1se_baseball = lambda_baseball[2]

lambda_min_online_news = lambda_online_news[1]
lambda_1se_online_news = lambda_online_news[2]

lambda_min_mobile_price = lambda_mobile_price[1]
lambda_1se_mobile_price = lambda_mobile_price[2]

lambda_min_churn = lambda_churn[1]
lambda_1se_churn = lambda_churn[2]
```


We find the best alpha value for Lasso Regression, and largest value of lambda such that error is within 1 standard error of the minimum. Now, we are ready to build a Lasso Regression for all datasets using _lasso_model_ function.

```{r lasso model}
lasso_min_baseball = lasso_model(baseball_data_train, "Batting.average", lambda_min_baseball, "gaussian")
lasso_1se_baseball = lasso_model(baseball_data_train, "Batting.average", lambda_1se_baseball, "gaussian")

lasso_min_online_news = lasso_model(online_news_data_train, "shares", lambda_min_online_news, "gaussian")
lasso_1se_online_news = lasso_model(online_news_data_train, "shares", lambda_1se_online_news, "gaussian")

lasso_min_mobile_price = lasso_model(mobile_price_data_train, "price_range", lambda_min_mobile_price, "multinomial")
lasso_1se_mobile_price = lasso_model(mobile_price_data_train, "price_range", lambda_1se_mobile_price, "multinomial")

lasso_min_churn = lasso_model(churn_data_train, "Churn", lambda_min_churn, "binomial")
lasso_1se_churn = lasso_model(churn_data_train, "Churn", lambda_1se_churn, "binomial")

```


Now, we have create 2 Lasso Regression models for each datasets, which first one is created with lambda_min and the other one is created with lambda_1se.

#### Decision Trees

We can create Decision Tree models. The most important parameters in a tree model are **the minimal number of observations per tree leaf** and **complexity parameter**. We need to find best parameters for all models individually.

```{r}
mape_baseball = 99999
mape_online_news = 99999
cm_mobile_price = 0
cm_churn = 0
for (minbucket in seq(2, 7, length.out = 6)){
  dt_cv_baseball = dt_tuning(baseball_data_train, "Batting.average", minbucket)
  predict_baseball = predict(dt_cv_baseball$finalModel)
  score_baseball = calculate_mape(predict_baseball, baseball_data_train$Batting.average)
  if (score_baseball < mape_baseball) {
    mape_baseball = score_baseball
    dt_baseball = dt_cv_baseball$finalModel
  }
  
  dt_cv_online_news = dt_tuning(online_news_data_train, "shares", minbucket)
  predict_online_news = predict(dt_cv_online_news$finalModel)
  score_online_news = calculate_mape(predict_online_news, online_news_data_train$shares)
  if (score_online_news < mape_online_news) {
    mape_online_news = score_online_news
    dt_online_news = dt_cv_online_news$finalModel
  }

  dt_cv_mobile_price = dt_tuning(mobile_price_data_train, "price_range", minbucket)
  predict_mobile_price = predict(dt_cv_mobile_price$finalModel)
  predict_mobile_price = apply(data.table(Class = apply(predict_mobile_price, 1, which.max)), 
                               1, function(x){unique(mobile_price_train$price_range)[order(unique(mobile_price_data_train$price_range))][x]})
  score_mobile_price = calculate_cm(predict_mobile_price, mobile_price_data_train$price_range)
  if (score_mobile_price > cm_mobile_price) {
    MAPE_mobile_price = score_mobile_price
    dt_mobile_price = dt_cv_mobile_price$finalModel
  }
  
  dt_cv_churn = dt_tuning(churn_data_train, "Churn", minbucket)
  predict_churn = predict(dt_cv_churn$finalModel)
  predict_churn = apply(data.table(Class = apply(predict_churn, 1, which.max)), 
                        1, function(x){unique(churn_data_train$Churn)[order(unique(churn_data_train$Churn))][x]})
  score_churn = calculate_cm(predict_churn, churn_data_train$Churn)
  if (score_churn > cm_churn) {
    MAPE_churn = score_churn
    dt_churn = dt_cv_churn$finalModel
  }
}
```


```{r}
rf_baseball = rf_tuning(baseball_data_train, "Batting.average")
rf_online_news = rf_tuning(online_news_data_train, "shares")
rf_mobile_price = rf_tuning(mobile_price_data_train, "price_range")
rf_churn = rf_tuning(churn_data_train, "Churn")
```


```{r}
churn_data_train_sgb = churn_data_train %>%
  mutate(Churn = if_else(Churn == "Yes", 1, 0))
churn_data_test_sgb = churn_data_test %>%
  mutate(Churn = if_else(Churn == "Yes", 1, 0))

grid_sgb <- expand.grid(shrinkage = c(.01, .05, .1), interaction.depth = c(3, 5, 7), n.trees = c(80, 100, 120), n.minobsinnode = 10, min_RMSE = 0)
sgb_baseball = gbm_tuning(baseball_data_train, "Batting.average", "gaussian", grid_sgb)
sgb_online_news = gbm_tuning(online_news_data_train, "shares", "gaussian", grid_sgb)
sgb_mobile_price = gbm_tuning(mobile_price_data_train, "price_range", "multinomial", grid_sgb)
sgb_churn = gbm_tuning(churn_data_train_sgb, "Churn", "bernoulli", grid_sgb)
```




#### Random Forests 

#### Stochastic Gradient Boosting

#### Comparison





### 3.2 Online News Popularity

### 3.3 Mobile Price Segmentation

### 3.4 Telco User Churn



